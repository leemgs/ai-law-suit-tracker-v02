from __future__ import annotations
import os
from datetime import datetime, timezone
from zoneinfo import ZoneInfo

from .fetch import fetch_news
from .extract import load_known_cases, build_lawsuits_from_news
from .render import render_markdown
from .github_issue import (
    find_or_create_issue,
    create_comment,
    close_other_daily_issues,
    get_issue_body,
    update_issue_body,
    issue_has_base_snapshot,
)
from .slack import post_to_slack
from .courtlistener import (
    search_recent_documents,
    build_complaint_documents_from_hits,
    build_case_summaries_from_hits,
    build_case_summaries_from_docket_numbers,
    build_case_summaries_from_case_titles,
    build_documents_from_docket_ids,
)
from .queries import COURTLISTENER_QUERIES

def main() -> None:
    # 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    owner = os.environ["GITHUB_OWNER"]
    repo = os.environ["GITHUB_REPO"]
    gh_token = os.environ["GITHUB_TOKEN"]
    slack_webhook = os.environ["SLACK_WEBHOOK_URL"]

    base_title = os.environ.get("ISSUE_TITLE_BASE", "AI ë¶ˆë²•/ë¬´ë‹¨ í•™ìŠµë°ì´í„° ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    lookback_days = int(os.environ.get("LOOKBACK_DAYS", "3"))
    # í•„ìš” ì‹œ 2ë¡œ ë³€ê²½: í™˜ê²½ë³€ìˆ˜ LOOKBACK_DAYS=2
    
    # KST ê¸°ì¤€ ë‚ ì§œ ìƒì„±
    now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
    run_ts_kst = now_kst.strftime("%Y-%m-%d %H:%M")
    issue_day_kst = now_kst.strftime("%Y-%m-%d")
    issue_title = f"{base_title} ({issue_day_kst})"
    print(f"KST ê¸°ì¤€ ì‹¤í–‰ì‹œê°: {run_ts_kst}")
    
    issue_label = os.environ.get("ISSUE_LABEL", "ai-lawsuit-monitor")

    # 1) CourtListener ê²€ìƒ‰
    hits = []
    for q in COURTLISTENER_QUERIES:
        hits.extend(search_recent_documents(q, days=lookback_days, max_results=20))
    
    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for h in hits:
        key = (h.get("absolute_url") or h.get("url") or "") + "|" + (h.get("caseName") or h.get("title") or "")
        dedup[key] = h
    hits = list(dedup.values())

    cl_docs = build_complaint_documents_from_hits(hits, days=lookback_days)
    # RECAP ë„ì¼“(ì‚¬ê±´) ìš”ì•½: "ë²•ì› ì‚¬ê±´(ë„ì¼“) í™•ì¸ ê±´ìˆ˜"ë¡œ ì‚¬ìš©
    cl_cases = build_case_summaries_from_hits(hits)

    # 2) ë‰´ìŠ¤ ìˆ˜ì§‘
    news = fetch_news()
    known = load_known_cases()
    lawsuits = build_lawsuits_from_news(news, known, lookback_days=lookback_days)

    # 2-1) ë‰´ìŠ¤ í…Œì´ë¸”ì˜ ì†Œì†¡ë²ˆí˜¸(ë„ì¼“ë²ˆí˜¸)ë¡œ RECAP ë„ì¼“/ë¬¸ì„œ í™•ì¥
    docket_numbers = [s.case_number for s in lawsuits if (s.case_number or "").strip() and s.case_number != "ë¯¸í™•ì¸"]
    extra_cases = build_case_summaries_from_docket_numbers(docket_numbers)

    # 2-2) ì†Œì†¡ë²ˆí˜¸ê°€ ì—†ë”ë¼ë„, 'ì†Œì†¡ì œëª©'(ì¶”ì • ì¼€ì´ìŠ¤ëª…)ìœ¼ë¡œ ë„ì¼“ í™•ì¥
    case_titles = [s.case_title for s in lawsuits if (s.case_title or "").strip() and s.case_title != "ë¯¸í™•ì¸"]
    extra_cases_by_title = build_case_summaries_from_case_titles(case_titles)

    merged_cases = {c.docket_id: c for c in (cl_cases + extra_cases + extra_cases_by_title)}
    cl_cases = list(merged_cases.values())

    # ë¬¸ì„œë„ docket id ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì‹œë„(Complaint ìš°ì„ , ì—†ìœ¼ë©´ fallback)
    docket_ids = list(merged_cases.keys())
    extra_docs = build_documents_from_docket_ids(docket_ids, days=lookback_days)
    merged_docs = {}
    for d in (cl_docs + extra_docs):
        key = (d.docket_id, d.doc_number, d.date_filed, d.document_url)
        merged_docs[key] = d
    cl_docs = list(merged_docs.values())

    docket_case_count = len(cl_cases)
    
    # =====================================================
    # ğŸ”¥ FIX: RECAP ë¬¸ì„œ ê±´ìˆ˜ ê³„ì‚° ë°©ì‹ ìˆ˜ì •
    # ê¸°ì¡´: len(cl_docs)
    # ë¬¸ì œ: HTML fallback ë“±ìœ¼ë¡œ CLCaseSummaryì—ë§Œ complaint_linkê°€ ìˆê³ 
    #       CLDocumentê°€ ìƒì„±ë˜ì§€ ì•ŠëŠ” ê²½ìš° KPIê°€ 0ìœ¼ë¡œ ë‚˜ì˜´
    # í•´ê²°: CLCaseSummary ê¸°ì¤€ìœ¼ë¡œ complaint_link ì¡´ì¬ ì—¬ë¶€ ì¹´ìš´íŠ¸
    # =====================================================
    recap_doc_count = sum(
        1 for c in cl_cases
        if (getattr(c, "complaint_link", "") or "").strip()
    )

    # 3) ë Œë”ë§
    md = render_markdown(
        lawsuits,
        cl_docs,
        cl_cases,
        recap_doc_count,
        lookback_days=lookback_days,
    )    
    md = f"### ì‹¤í–‰ ì‹œê°(KST): {run_ts_kst}\n\n" + md
    
    print("===== REPORT BEGIN =====")
    print(md[:1000]) # ë¡œê·¸ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦¬ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥
    print("===== REPORT END =====")

    # 4) GitHub Issue ì‘ì—…
    issue_no = find_or_create_issue(owner, repo, gh_token, issue_title, issue_label)
    issue_url = f"https://github.com/{owner}/{repo}/issues/{issue_no}"
 
    # =====================================================
    # ğŸ”¥ Base Snapshot ë¹„êµ ë¡œì§
    # =====================================================
    current_body = get_issue_body(owner, repo, gh_token, issue_no)

    skipped_count = 0

    if not issue_has_base_snapshot(current_body):
        # ğŸ•˜ ìµœì´ˆ ì‹¤í–‰ â†’ ì „ì²´ ë¦¬í¬íŠ¸ë¥¼ ë³¸ë¬¸ìœ¼ë¡œ ì €ì¥
        update_issue_body(owner, repo, gh_token, issue_no, md)
        print("ìµœì´ˆ ì‹¤í–‰ â†’ Issue ë³¸ë¬¸ì„ base snapshotìœ¼ë¡œ ì €ì¥")
    else:
        # ğŸ•‘ ì¬ì‹¤í–‰ â†’ base snapshotê³¼ ë¹„êµ
        base_lines = set(current_body.splitlines())
        new_lines = []

        for line in md.splitlines():
            if line in base_lines:
                skipped_count += 1
                new_lines.append("skip")
            else:
                new_lines.append(line)

        summary_block = (
            "## ğŸ”„ ë‹¹ì¼ ì¬ì‹¤í–‰ ë³€ê²½ ìš”ì•½\n\n"
            f"- ğŸ“° ì™¸ë¶€ ê¸°ì‚¬ ì‹ ê·œ: {len(lawsuits)}ê±´\n"
            f"- âš–ï¸ RECAP ì‹ ê·œ ì‚¬ê±´: {docket_case_count}ê±´\n"
            f"- ğŸ“„ RECAP ì‹ ê·œ ë¬¸ì„œ: {recap_doc_count}ê±´\n"
            f"- ğŸ” ê¸°ì¡´ ë‚´ìš© ìƒëµ: {skipped_count}ê±´\n\n"
            "---\n"
        )

        md = summary_block + "\n".join(new_lines)
   
    # ì´ì „ ë‚ ì§œ ì´ìŠˆ Close
    closed_nums = close_other_daily_issues(owner, repo, gh_token, issue_label, base_title, issue_title, issue_no, issue_url)
    if closed_nums:
        print(f"ì´ì „ ë‚ ì§œ ì´ìŠˆ ìë™ Close: {closed_nums}")
    
    # KST ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M KST")

    comment_body = f"\n\n{md}"
    create_comment(owner, repo, gh_token, issue_no, comment_body)
    print(f"Issue #{issue_no} ëŒ“ê¸€ ì—…ë¡œë“œ ì™„ë£Œ")

    # 5) Slack ìš”ì•½ ì „ì†¡
    summary_lines = [
        f"*AI ì†Œì†¡ ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸*",
        f"- ğŸ“° ì‹ ê·œ ê¸°ì‚¬: {len(lawsuits)}ê±´",
        f"- âš–ï¸ ì‹ ê·œ RECAP ì‚¬ê±´: {docket_case_count}ê±´",
        f"- ğŸ” ê¸°ì¡´ ë‚´ìš© ìƒëµ: {skipped_count}ê±´",
        f"- ğŸ‘‰ GitHub Issue: <{issue_url}|#{issue_no}>",
    ]
    
    if cl_docs:
        # date_filed ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        top = sorted(cl_docs, key=lambda x: getattr(x, 'date_filed', ''), reverse=True)[:3]
        summary_lines.append("- ìµœì‹  RECAP ë¬¸ì„œ:")
        for d in top:
            date = getattr(d, 'date_filed', 'N/A')
            name = getattr(d, 'case_name', 'Unknown Case')
            summary_lines.append(f"  â€¢ {date} | {name}")
    
    post_to_slack(slack_webhook, "\n".join(summary_lines))
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    main()
